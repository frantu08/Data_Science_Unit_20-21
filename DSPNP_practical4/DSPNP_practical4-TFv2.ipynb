{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 4: Getting Started with Deep Learning Models in TensorFlow\n",
    "\n",
    "*This notebook is based on past years' notebooks by Marek Rei and Guy Emerson*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical will cover a few different network architectures and we will look at different components that are often used in neural networks in practice. It will also allow you to learn more about [`TensorFlow`](https://www.tensorflow.org), a popular open-source machine learning and deep learning library. I'd also recommend checking the `TensorFlow` documentation to learn more about the rich functionality of this toolkit.\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "In this practical you will learn about:\n",
    "- The basics of running `TensorFlow` \n",
    "- How to implement a feedforward neural network in Python\n",
    "- How to visualise your network architecture using `TensorBoard` and track changes \n",
    "- How to apply deep learning to both classification and regression tasks.\n",
    "\n",
    "**Additional references**: Aurelien Geron, *Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow*.\n",
    "\n",
    "Before we start, let's import the usual libraries as we did in previous practicals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import `TensorFlow` into our notebook. Note: `TensorFlow` v2 was released last year, and it mostly relies on `Keras` interpretative module fit on top of it. As a result, it is much more interpretable and user-friendly than v1, however if you want to better understand the inner workings of `TensorFlow` you are welcome to check the accompanying notebook [`DSPNP_practical4-TFv1.ipynb`](./DSPNP_practical4-TFv1.ipynb): even if you are using `TensorFlow2`, you can still switch to using v1 API, which is available as a submodule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal TensorFlow Example\n",
    "\n",
    "In this example, we create a simple network that takes an input vector, multiplies it by a weight matrix, adds a weight vector, and returns the result.\n",
    "\n",
    "`tf.Variable` defines model parameters, which can be trained (as we will see shortly). Here, we initialise the matrix variable as a 3x3 matrix, with every entry as 1 (`tf.ones`). Meanwhile, we initialise the 3x1 vector variable with every entry as 0 (`tf.zeros`). `tf.linalg.matvec` multiplies a matrix and a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([12. 12. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = tf.Variable(tf.ones(shape=(3,3)))\n",
    "weight_vector = tf.Variable(tf.zeros(shape=(3,)))\n",
    "\n",
    "def affine_transformation(input_vector):\n",
    "    return tf.linalg.matvec(weight_matrix, input_vector) + weight_vector\n",
    "\n",
    "result = affine_transformation([2.,3.,7.])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following [reset function](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) is often useful. It is necessary to reset the `TensorFlow` network from time to time: as we have many different small networks in one notebook and we don't want them interfering with each other, as a pre-emptive measure we will occasionally reset the computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Parameters\n",
    "\n",
    "This example shows how to optimise the parameters in your model.\n",
    "\n",
    "We first define a network that takes an input vector, multiplies it with a matrix (as defined above), and sums the elements of the resulting vector (using `tf.math.reduce_sum`). We then define a loss function as the square error. Given a specific input and output, we can calculate the loss of applying the network to the input.\n",
    "\n",
    "Next, we define an optimiser â€“ here, we are using *stochastic gradient descent* (*SGD*) with the learning rate $0.001$. We then use this optimiser to train this network for $10$ epochs, over this single training point. This optimises the output towards the target value $20$. Printing out the results, we can see that the output gradually moves towards the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(29.952, shape=(), dtype=float32)\n",
      "tf.Tensor(26.190144, shape=(), dtype=float32)\n",
      "tf.Tensor(23.850271, shape=(), dtype=float32)\n",
      "tf.Tensor(22.39487, shape=(), dtype=float32)\n",
      "tf.Tensor(21.489607, shape=(), dtype=float32)\n",
      "tf.Tensor(20.926535, shape=(), dtype=float32)\n",
      "tf.Tensor(20.576305, shape=(), dtype=float32)\n",
      "tf.Tensor(20.358461, shape=(), dtype=float32)\n",
      "tf.Tensor(20.222961, shape=(), dtype=float32)\n",
      "tf.Tensor(20.138683, shape=(), dtype=float32)\n",
      "tf.Tensor(20.086262, shape=(), dtype=float32)\n",
      "tf.Tensor(20.053656, shape=(), dtype=float32)\n",
      "tf.Tensor(20.033375, shape=(), dtype=float32)\n",
      "tf.Tensor(20.020758, shape=(), dtype=float32)\n",
      "tf.Tensor(20.01291, shape=(), dtype=float32)\n",
      "tf.Tensor(20.00803, shape=(), dtype=float32)\n",
      "tf.Tensor(20.004995, shape=(), dtype=float32)\n",
      "tf.Tensor(20.003109, shape=(), dtype=float32)\n",
      "tf.Tensor(20.001932, shape=(), dtype=float32)\n",
      "tf.Tensor(20.001202, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000748, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000465, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000286, shape=(), dtype=float32)\n",
      "tf.Tensor(20.00018, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000114, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000069, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000044, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000027, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000017, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000011, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000006, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000002, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000002, shape=(), dtype=float32)\n",
      "tf.Tensor(20.000002, shape=(), dtype=float32)\n",
      "tf.Tensor(20.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "weight_matrix = tf.Variable(tf.ones(shape=(3,3)))\n",
    "weight_vector = tf.Variable(tf.zeros(shape=(3,)))\n",
    "\n",
    "def network(input_vector):\n",
    "    return tf.math.reduce_sum(affine_transformation(input_vector))\n",
    "\n",
    "def loss_fn(predicted, gold):\n",
    "    return tf.square(predicted - gold)\n",
    "\n",
    "input = [2.,3.,7.]\n",
    "gold_output = 20\n",
    "\n",
    "def loss():\n",
    "    return loss_fn(network(input), gold_output)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(35):\n",
    "    opt.minimize(loss, var_list=[weight_matrix, weight_vector])\n",
    "    print(network(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: Try changing the learning rate and the number of epochs. What results are you getting?"
   ]
  },
  {
   "source": [
    "- Initial result with lr = 1e-3 and #epoch = 10\n",
    "\n",
    "tf.Tensor(29.952, shape=(), dtype=float32)\n",
    "tf.Tensor(26.190144, shape=(), dtype=float32)\n",
    "tf.Tensor(23.850271, shape=(), dtype=float32)\n",
    "tf.Tensor(22.39487, shape=(), dtype=float32)\n",
    "tf.Tensor(21.489607, shape=(), dtype=float32)\n",
    "tf.Tensor(20.926535, shape=(), dtype=float32)\n",
    "tf.Tensor(20.576305, shape=(), dtype=float32)\n",
    "tf.Tensor(20.358461, shape=(), dtype=float32)\n",
    "tf.Tensor(20.222961, shape=(), dtype=float32)\n",
    "tf.Tensor(20.138683, shape=(), dtype=float32)\n",
    "\n",
    "- With lr = 1e-2 and 10 epoch\n",
    "tf.Tensor(-24.48, shape=(), dtype=float32)\n",
    "tf.Tensor(143.6544, shape=(), dtype=float32)\n",
    "tf.Tensor(-323.75922, shape=(), dtype=float32)\n",
    "tf.Tensor(975.6506, shape=(), dtype=float32)\n",
    "tf.Tensor(-2636.7085, shape=(), dtype=float32)\n",
    "tf.Tensor(7405.6504, shape=(), dtype=float32)\n",
    "tf.Tensor(-20512.11, shape=(), dtype=float32)\n",
    "tf.Tensor(57099.266, shape=(), dtype=float32)\n",
    "tf.Tensor(-158660.36, shape=(), dtype=float32)\n",
    "tf.Tensor(441151.38, shape=(), dtype=float32)\n",
    "\n",
    "-- Overshooting big time\n",
    "\n",
    "- Back to lr = 1e-3. Try 20 epochs\n",
    "\n",
    " last = tf.Tensor(20.001202, shape=(), dtype=float32) =  improvement\n",
    "\n",
    "-- 100 epochs = converting to 20 pretty early on \n",
    "\n",
    "-- 40 epoch = more than enough for converting\n",
    "\n",
    "-- 35 enough\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Layers\n",
    "\n",
    "For most cases, we don't actually need to create the trainable variables manually. Instead, the feedfoward layer is available as a pre-defined module.\n",
    "\n",
    "We can define a network as a sequence of operations, using [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential). The first operation here is a dense feedforward layer (`tf.keras.layers.Dense`), which acts like the `affine_transfomation` function we defined earlier. The second operation sums the elements of the vector â€“ this isn't a standard operation, so we use `tf.keras.layers.Lambda` to allow a user-defined function.\n",
    "\n",
    "By default, the parameters in a layer (like [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) are initialised randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_shape=(3,)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that such a model expects the input data to be given as a *minibatch* â€“ this means that the input tensor should have an extra index, which ranges over datapoints. In our case, instead of passing a 3-dimensional input vector, we have to pass an Nx3 matrix, where N is the number of datapoints. Here, we can apply the model to a single datapoint (a 1x3 matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-2.3992734], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.predict(tf.constant([[2.,3.,7.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model defined in terms of layers, let's replace the manually created variables of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([9.302681], shape=(1,), dtype=float32)\ntf.Tensor([13.346269], shape=(1,), dtype=float32)\ntf.Tensor([15.861379], shape=(1,), dtype=float32)\ntf.Tensor([17.425777], shape=(1,), dtype=float32)\ntf.Tensor([18.398834], shape=(1,), dtype=float32)\ntf.Tensor([19.004074], shape=(1,), dtype=float32)\ntf.Tensor([19.380537], shape=(1,), dtype=float32)\ntf.Tensor([19.61469], shape=(1,), dtype=float32)\ntf.Tensor([19.76034], shape=(1,), dtype=float32)\ntf.Tensor([19.850931], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_shape=(3,)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=1))\n",
    "])\n",
    "\n",
    "def loss_fn(predicted, gold):\n",
    "    return tf.square(predicted - gold)\n",
    "\n",
    "input = tf.constant([[2.,3.,7.]])\n",
    "gold_output = 20\n",
    "\n",
    "def loss():\n",
    "    return loss_fn(model(input), gold_output)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    opt.minimize(loss, var_list=model.trainable_variables)\n",
    "    print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, for standard optimizers and loss functions, the `TensorFlow` API makes it even easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(10.422389, shape=(), dtype=float32)\ntf.Tensor(14.0427265, shape=(), dtype=float32)\ntf.Tensor(16.294575, shape=(), dtype=float32)\ntf.Tensor(17.695225, shape=(), dtype=float32)\ntf.Tensor(18.56643, shape=(), dtype=float32)\ntf.Tensor(19.108322, shape=(), dtype=float32)\ntf.Tensor(19.445374, shape=(), dtype=float32)\ntf.Tensor(19.655022, shape=(), dtype=float32)\ntf.Tensor(19.785423, shape=(), dtype=float32)\ntf.Tensor(19.866533, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_shape=(3,)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3), # alternatively, optimizer=`sgd`\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "input = tf.constant([[2.,3.,7.]])\n",
    "gold_output = tf.constant([[20.]])\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train_on_batch(input, gold_output)\n",
    "    print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "As you saw in the previous lectures, activation functions are what gives neural networks their power to model non-linear patterns in the data. After applying an affine transformation, we then apply a non-linear activation function to each element. There are a number of different activation functions to choose from.\n",
    "\n",
    "The [sigmoid function](https://en.wikipedia.org/wiki/Logistic_function), also known as the logistic function, is the most classic non-linear activation. It transforms the value to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.layers.Dense(100, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modern networks, the [tanh function](https://en.wikipedia.org/wiki/Hyperbolic_function) is used more often. It has more flexibility, as it transforms the input value to a range between -1 and 1, and can therefore output negative values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.layers.Dense(100, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular one is the [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) function, or the ReLU. This function acts as a linear function above zero, but restricts everything below zero to 0. By doing this it also introduces non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.keras.layers.Dense(100, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial linear property of the ReLU can help it converge faster on some tasks, although in practice tanh may be a more robust option.\n",
    "\n",
    "Finally, for classification tasks [softmax](https://en.wikipedia.org/wiki/Softmax_function) is an important activation function. Unlike the activation functions mentioned above, it isn't applied to each element separately. It converts a vector of scores into a probability distribution: after applying the softmax, all values are between 0 and 1, and together they sum to 1. Higher scores are assigned to higher probabilities, via the formula:\n",
    "\n",
    "\n",
    "$P(i) \\propto \\exp(x_i)$\n",
    "\n",
    "\n",
    "Or, more explicitly:\n",
    "\n",
    "$P(i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "Notice how the value of the denominator depends on all other values.\n",
    "\n",
    "The softmax is often used in the output layer of a network performing classification, in order to predict a probability distribution over all the possible classes. For example, the following model takes a 20-dimensional input, maps it to a 50-dimensional hidden layer, then maps it to a distribution over 10 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, input_shape=(20,), activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations and Useful Functions\n",
    "\n",
    "`TensorFlow` has corresponding versions of all the main operations you might want to use. This means you can add them into your computation graph and into your neural network. The most common operations are available in `tf`, and further operations are available in `tf.math`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.exp(x, name=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "tf.abs # absolute value\n",
    "tf.negative # computes the negative value\n",
    "tf.sign # returns 1, 0 or -1 depending on the sign of the input\n",
    "tf.math.reciprocal # reciprocal 1/x\n",
    "tf.square # return input squared\n",
    "tf.round # return rounded value\n",
    "tf.sqrt # square root\n",
    "tf.math.rsqrt # reciprocal of square root\n",
    "tf.pow # power\n",
    "tf.exp # exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations can be applied to scalar values, but also to vectors, matrices and higher-order tensors. In the latter case, they will be applied element-wise. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([-3.2  2.7], shape=(2,), dtype=float32)\ntf.Tensor([2.25      4.4099994], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.negative([3.2,-2.7]))\n",
    "print(tf.square([1.5,-2.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful operations are performed over a whole vector/matrix tensor and return a single value (e.g., you saw `tf.reduce_sum` earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.argmin_v2(input, axis=None, output_type=tf.int64, name=None)>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "tf.reduce_sum # Add elements together\n",
    "tf.reduce_mean # Average over elements\n",
    "tf.reduce_min # Minimum value\n",
    "tf.reduce_max # Maximum value\n",
    "tf.argmax # Index of the largest value\n",
    "tf.argmin # Index of the smallest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Learning Rates\n",
    "\n",
    "Above, we used stochastic gradient descent (SGD) to train our model. This uses a fixed learning rate to update the parameters. Several optimisation algorithms are based on SGD, but adaptively adjust the learning rate (usually for each parameter separately).\n",
    "\n",
    "Different adaptive learning rate strategies are also implemented in `TensorFlow` as functions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tf.keras.optimizers.SGD\n",
    "tf.keras.optimizers.Adadelta\n",
    "tf.keras.optimizers.Adam\n",
    "tf.keras.optimizers.RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in the differences between these strategies, [this blog post](http://ruder.io/optimizing-gradient-descent/) provides more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an XOR Function\n",
    "\n",
    "[XOR](https://en.wikipedia.org/wiki/XOR_gate) is the function that takes two binary values and returns 1 only if one of them is 1 and the other 0, while returning 0 if both of them have the same value. It can be a difficult function to learn and cannot be modelled with a linear model. But let's try anyway.\n",
    "\n",
    "Our dataset consists of all the possible different states that XOR can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_input = tf.constant([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "xor_output = tf.constant([0.0, 1.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a linear network and optimize it on this dataset, printing out the predictions at each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after 10 epochs: [ 0.38239497  1.1411629  -0.19967383  0.5590941 ]\n",
      "after 20 epochs: [0.41981086 0.8885872  0.08577871 0.55455506]\n",
      "after 30 epochs: [0.4418205  0.73120046 0.25052935 0.53990924]\n",
      "after 40 epochs: [0.45770857 0.63726085 0.34946534 0.5290176 ]\n",
      "after 50 epochs: [0.46925607 0.58133227 0.40901846 0.5210947 ]\n",
      "after 60 epochs: [0.47765055 0.54807806 0.4449074  0.51533484]\n",
      "after 70 epochs: [0.48375294 0.5283364  0.4665643  0.51114774]\n",
      "after 80 epochs: [0.4881891  0.5166391  0.4796539  0.50810397]\n",
      "after 90 epochs: [0.49141398 0.5097248  0.48758036 0.5058912 ]\n",
      "after 100 epochs: [0.49375835 0.5056498  0.49239114 0.50428265]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "linear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(2,))\n",
    "])\n",
    "\n",
    "linear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "                     loss='mean_squared_error')\n",
    "\n",
    "for epoch in range(100):\n",
    "    linear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), linear_model(xor_input).numpy().reshape((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not doing very well. Ideally, the predictions should be [0, 1, 1, 0], but in this case they are hovering around 0.5 for every input case.\n",
    "\n",
    "In order to improve this architecture, let's add some non-linear layers into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after 10 epochs: [0.41006714 0.61654097 0.44717988 0.5172276 ]\n",
      "after 20 epochs: [0.36594415 0.6375043  0.50042313 0.5196598 ]\n",
      "after 30 epochs: [0.31471503 0.6648598  0.53923887 0.508127  ]\n",
      "after 40 epochs: [0.27109194 0.6924951  0.5787459  0.4759154 ]\n",
      "after 50 epochs: [0.23768663 0.7182015  0.6243684  0.4246406 ]\n",
      "after 60 epochs: [0.21054459 0.74415994 0.6739305  0.3657127 ]\n",
      "after 70 epochs: [0.18618777 0.7708974  0.7200841  0.31136337]\n",
      "after 80 epochs: [0.16435179 0.79640496 0.7585563  0.26679623]\n",
      "after 90 epochs: [0.14564517 0.8189459  0.7892319  0.23174155]\n",
      "after 100 epochs: [0.13009909 0.83804125 0.81351274 0.20427725]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, input_shape=(2,), activation='tanh'), # note that these settings can be changed\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='mean_squared_error')\n",
    "\n",
    "for epoch in range(100):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy().reshape((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better. The values are much closer to [0, 1, 1, 0] than before, and they will continue improving if we train for longer. (Remember that the model is initialised randomly â€“ if you run it a few times, you will see that the results vary with each run. Check the [documentation](https://www.tensorflow.org/tutorials/keras/save_and_load) on how you can save and restore a particular model).\n",
    "\n",
    "We also had to increase the learning rate for this network. It would still be learning with a smaller learning rate, but it would be converging very slowly. As we discussed in the lectures, learning rate is a hyperparameter that can vary quite a bit depending on the network architecture and dataset.\n",
    "\n",
    "**Optional**: Try changing various settings in the current network, e.g. *width* (number of neurons per layer), *depth* (number of layers), *activation functions* applied to each layer, and number of *epochs*. What changes do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after 100 epochs: [0.03267092 0.97846866 0.98246634 0.01838118]\n",
      "after 200 epochs: [0.02090898 0.98613966 0.9884274  0.01144603]\n",
      "after 300 epochs: [0.01642162 0.9890651  0.9907808  0.00888869]\n",
      "after 400 epochs: [0.01390955 0.9906928  0.9921193  0.00748086]\n",
      "after 500 epochs: [0.01225176 0.99175763 0.9930015  0.0065603 ]\n",
      "after 600 epochs: [0.01105824 0.99253273 0.99364126 0.00590658]\n",
      "after 700 epochs: [0.01014513 0.99311876 0.99412924 0.00540593]\n",
      "after 800 epochs: [0.00941977 0.99358654 0.99451804 0.0050118 ]\n",
      "after 900 epochs: [0.00882775 0.993968   0.9948312  0.00469002]\n",
      "after 1000 epochs: [0.00833234 0.9942883  0.99509466 0.00442249]\n"
     ]
    }
   ],
   "source": [
    "# Mine\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(16, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(8, input_shape=(2,), activation='tanh'), # note that these settings can be changed\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='mean_squared_error')\n",
    "\n",
    "for epoch in range(1000):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy().reshape((4,)))"
   ]
  },
  {
   "source": [
    "- tf.keras.layers.Dense(100, input_shape=(2,), activation='relu') - got after 100 epochs: [0.15288806 0.91357195 0.910642   0.06977707]\n",
    "\n",
    "- tf.keras.layers.Dense(100, input_shape=(2,), activation='sigmoid') got after 100 epochs: [0.7675986  0.7659911  0.74394864 0.74212164] or after 100 epochs: [0.2369608  0.24330732 0.2336308  0.2398197 ] = weird\n",
    "\n",
    "- with more layers \n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "   \n",
    "    tf.keras.layers.Dense(16, input_shape=(2,), activation='tanh'),\n",
    "   \n",
    "    tf.keras.layers.Dense(8, input_shape=(2,), activation='tanh'), # note that these settings can be changed\n",
    "   \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    " \n",
    "\n",
    " after 100 epochs: [0.07005394 0.90713346 0.90309864 0.0912008 ]\n",
    "\n",
    "\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(16, input_shape=(2,), activation='tanh'),\n",
    "    \n",
    "    tf.keras.layers.Dense(8, input_shape=(2,), activation='relu'), # note that these settings can be changed\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "after 100 epochs: [0.0411101  0.9592626  0.96524894 0.04195771]\n",
    "\n",
    "\n",
    "- best I got with 1000 epoch\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Dense(16, input_shape=(2,), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(8, input_shape=(2,), activation='tanh'), # note that these settings can be changed\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "after 1000 epochs: [0.01317123 0.99075276 0.991938   0.0065771 ]\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Classification\n",
    "\n",
    "We can also do classification with `TensorFlow`. For this, we often use the softmax activation function described above, which predicts the probability for each of the possible classes.\n",
    "\n",
    "We also have to change the loss function, as squared error is not suitable for classification. A suitable loss function is [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy). Since the correct output has probability 1 for the correct class, and probability 0 for the rest, minimising cross entropy is the same as minimising the negative log probability of the correct class for each datapoint. In other words, by minimising cross entropy, we are trying to find the maximum likelihood model, which assigns high values for the correct label.\n",
    "\n",
    "We can change the XOR example above to perform classification instead. In this case, we are constructing a binary classifier â€“ choosing between the classes of 0 and 1. The output here prints the predicted probabilities of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after 10 epochs:\n[[0.73040897 0.26959106]\n [0.30164587 0.6983542 ]\n [0.29251486 0.7074852 ]\n [0.8380407  0.16195932]]\nafter 20 epochs:\n[[0.8464424  0.1535576 ]\n [0.07283828 0.92716175]\n [0.05179298 0.94820696]\n [0.9258144  0.07418562]]\nafter 30 epochs:\n[[0.9176367  0.08236331]\n [0.0360285  0.9639715 ]\n [0.03318096 0.966819  ]\n [0.9709073  0.02909266]]\nafter 40 epochs:\n[[0.94553566 0.0544643 ]\n [0.02526194 0.974738  ]\n [0.02019498 0.97980505]\n [0.98111504 0.018885  ]]\nafter 50 epochs:\n[[0.9602165  0.03978353]\n [0.01557026 0.9844297 ]\n [0.01458631 0.9854137 ]\n [0.98691326 0.01308666]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "for epoch in range(50):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert these probabilities into class predictions and also report some of the more familiar [evaluation metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics), e.g. *accuracy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "After 10 epochs: 0 1 1 0\n",
      "1/1 - 0s - loss: 0.5648 - accuracy: 1.0000\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "After 20 epochs: 0 1 1 0\n",
      "1/1 - 0s - loss: 0.4645 - accuracy: 1.0000\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "After 30 epochs: 0 1 1 0\n",
      "1/1 - 0s - loss: 0.3991 - accuracy: 1.0000\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "After 40 epochs: 0 1 1 0\n",
      "1/1 - 0s - loss: 0.3694 - accuracy: 1.0000\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "After 50 epochs: 0 1 1 0\n",
      "1/1 - 0s - loss: 0.3532 - accuracy: 1.0000\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])  \n",
    "\n",
    "for epoch in range(50):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    predictions = nonlinear_model.predict(xor_input)\n",
    "    result = tf.argmax(predictions, axis=1)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('\\nAfter {} epochs:'.format(epoch+1), \" \".join([str(x) for x in result.numpy()]))\n",
    "        test_loss, test_acc = nonlinear_model.evaluate(xor_input, xor_output, verbose=2)\n",
    "        print('\\nAccuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see in this printout that the model starts off with incorrect predictions, but fairly soon learns to return the correct sequence of [0, 1, 1, 0].\n",
    "\n",
    "Finally, here is how you can print out the confusion matrix. Since we are looking into a simple case here and the predictions from above are quite accurate, there is not much to be learned from the confusion matrix at this point (but note that this functionality may come in handy later in your practical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2 0]\n [0 2]]\n"
     ]
    }
   ],
   "source": [
    "conf_mx = tf.math.confusion_matrix(xor_output, result.numpy()).numpy()\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatching\n",
    "\n",
    "For the XOR data, there are only 4 datapoints. However, with realistic datasets, it is inefficient to train on the whole dataset at once, because this will require a lot of computation in order to make a single update step. \n",
    "\n",
    "Instead, we can train on a batch of data at a time. For example, here is how you can take batches of 2 datapoints for the XOR data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after 10 epochs:\n",
      "[[0.54673535 0.45326465]\n",
      " [0.6229881  0.3770119 ]\n",
      " [0.07548648 0.9245136 ]\n",
      " [0.8807345  0.11926548]]\n",
      "after 20 epochs:\n",
      "[[0.54479325 0.45520678]\n",
      " [0.54479325 0.45520678]\n",
      " [0.02473736 0.9752626 ]\n",
      " [0.95337    0.04663004]]\n",
      "after 30 epochs:\n",
      "[[0.49498877 0.5050112 ]\n",
      " [0.49498877 0.5050112 ]\n",
      " [0.01065516 0.9893448 ]\n",
      " [0.96953726 0.03046276]]\n",
      "after 40 epochs:\n",
      "[[0.5018401  0.49815992]\n",
      " [0.5018401  0.49815992]\n",
      " [0.00609661 0.99390334]\n",
      " [0.991812   0.00818798]]\n",
      "after 50 epochs:\n",
      "[[0.50322914 0.49677086]\n",
      " [0.50322914 0.49677086]\n",
      " [0.00729917 0.9927008 ]\n",
      " [0.98884016 0.01115981]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "for epoch in range(50):\n",
    "    for i in range(0,len(xor_input),BATCH_SIZE):\n",
    "        input_batch = xor_input[i:i+BATCH_SIZE]\n",
    "        output_batch = xor_output[i:i+BATCH_SIZE]\n",
    "        nonlinear_model.train_on_batch(input_batch, output_batch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this kind of functionality is built into `TensorFlow`. The following code trains the model with the given batch size and number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1118\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 513us/step - loss: 0.7360\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.6617\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.6344\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.5849\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 500us/step - loss: 0.5226\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.5071\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.5182\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.7638\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.6855\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.7459\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.4157\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2405\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2321\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 999us/step - loss: 0.2775\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.1262\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0993\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.1038\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.1027\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0614\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0579\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0498\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0441\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 505us/step - loss: 0.0378\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.0423\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0395\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0352\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0318\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 496us/step - loss: 0.0291\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 499us/step - loss: 0.0264\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.0244\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0221\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.0242\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0219\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 502us/step - loss: 0.0202\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0191\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0184\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.0176\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0172\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0167\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0158\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 995us/step - loss: 0.0152\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0144\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0148\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 983us/step - loss: 0.0138\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0132\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0128\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.0124\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.0120\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 498us/step - loss: 0.0122\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "final loss: 0.011365460231900215\n",
      "final predictions:\n",
      "[[0.9784761  0.02152384]\n",
      " [0.00540052 0.99459946]\n",
      " [0.00561601 0.99438393]\n",
      " [0.98742384 0.01257619]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "nonlinear_model.fit(xor_input, xor_output, batch_size=2, epochs=50)\n",
    "\n",
    "print('final loss:', nonlinear_model.evaluate(xor_input, xor_output))\n",
    "print('final predictions:', nonlinear_model.predict(xor_input), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "So far, you have been exploring the results using simple print out messages. However, neural networks can grow very large and complicated, and you may wish to visualise and explore various components along the way. Visualisation in this case is not only a useful method for reporting and sharing your results, but also a good way to inspect your network and debug it. \n",
    "\n",
    "[`TensorBoard`](https://www.tensorflow.org/tensorboard) provides you with all the needed visualisation functionality and allows you to:\n",
    "\n",
    "- track and visualise metrics such as loss and accuracy;\n",
    "- visualise the model graph (ops and layers);\n",
    "- view histograms of weights, biases, or other tensors as they change over time;\n",
    "- project embeddings to a lower dimensional space;\n",
    "- display images, text, and audio data;\n",
    "- profile TensorFlow programs;\n",
    "\n",
    "among other things. Moreover, you can run it in your browser or embed it directly into your notebook as the code below shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you will likely be introducing changes into your network and rerunning your code, it's important to be able to distinguish between these different runs to track the changes. Every time you run a new model, it will be stored in log files and added to your `TensorBoard`, so a good way to distinguish between various models is to add a time stamp to each of them. Let's add this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make sure you clean all the previous logs (e.g., if you've run this notebook before). You can clear any logs from previous runs by running `rm -rf ./logs/` from within your notebook folder in your terminal.\n",
    "\n",
    "Once this is done, let's train a network and store its details in the log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8301WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1077WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.7677s). Check your callbacks.\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 2.1077 - val_loss: 0.7892\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.8304 - val_loss: 0.6975\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2674 - val_loss: 0.7304\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7937 - val_loss: 0.6969\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0666 - val_loss: 0.7295\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7744 - val_loss: 0.6981\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7462 - val_loss: 0.6958\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7361 - val_loss: 0.6933\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0173 - val_loss: 0.7224\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.0531 - val_loss: 0.7386\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7688 - val_loss: 0.6975\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7402 - val_loss: 0.6941\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0120 - val_loss: 0.7229\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9900 - val_loss: 0.7047\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0303 - val_loss: 0.7309\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0609 - val_loss: 0.7415\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7731 - val_loss: 0.6946\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0149 - val_loss: 0.7238\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7556 - val_loss: 0.6941\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7200 - val_loss: 0.6932\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7121 - val_loss: 0.6931\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7075 - val_loss: 0.6931\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7050 - val_loss: 0.6933\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7232 - val_loss: 0.6931\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7161 - val_loss: 0.6931\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0178 - val_loss: 0.7206\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7249 - val_loss: 0.6945\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7045 - val_loss: 0.6932\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0050 - val_loss: 0.7193\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0492 - val_loss: 0.7375\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0675 - val_loss: 0.7437\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0734 - val_loss: 0.7456\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.9949 - val_loss: 0.7007\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7064 - val_loss: 0.6935\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6995 - val_loss: 0.6932\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6978 - val_loss: 0.6931\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6967 - val_loss: 0.6931\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.0047 - val_loss: 0.7198\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7132 - val_loss: 0.6948\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7100 - val_loss: 0.6932\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6959 - val_loss: 0.6932\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0037 - val_loss: 0.7194\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9896 - val_loss: 0.7055\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7031 - val_loss: 0.6939\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0089 - val_loss: 0.7223\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9898 - val_loss: 0.7048\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7141 - val_loss: 0.6938\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7035 - val_loss: 0.6932\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6950 - val_loss: 0.6931\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7007 - val_loss: 0.6931\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6931\n",
      "final loss: 0.6931493282318115\n",
      "final predictions:\n",
      "[[0.501077   0.49892306]\n",
      " [0.5010435  0.49895644]\n",
      " [0.5010115  0.49898848]\n",
      " [0.50097793 0.4990221 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "nonlinear_model.fit(xor_input, xor_output, \n",
    "                    batch_size=2, epochs=50, \n",
    "                    validation_data=(xor_input, xor_output),\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "print('final loss:', nonlinear_model.evaluate(xor_input, xor_output))\n",
    "print('final predictions:', nonlinear_model.predict(xor_input), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can explore your model in `TensorBoard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 27524), started 2:02:44 ago. (Use '!kill 27524' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore both the results (e.g., learning curves) under the `Scalars` tab and the network architecture itself under the `Graphs` tab. All visualisations are interactive â€“ note that you can scroll in on the network components in the `Graph` visualisation and double-click on the \"+\" sign in the upper right corner of any component to track operations, weights, etc.\n",
    "\n",
    "A brief overview of the dashboards from [`TensorBoard` documentation](https://www.tensorflow.org/tensorboard/get_started):\n",
    "\n",
    "- The `Scalars` dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
    "- The `Graphs` dashboard helps you visualise your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly.\n",
    "- The `Distributions` and `Histograms` dashboards show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way.\n",
    "\n",
    "There are additional `TensorBoard` plugins, which are automatically enabled when you log other types of data (note, it is not applicable to this notebook, as you are not working with any other types of data here). For example, the Keras `TensorBoard` callback lets you log images and embeddings as well. You can see what other plugins are available in `TensorBoard` by clicking on the \"inactive\" dropdown towards the top right.\n",
    "\n",
    "\n",
    "# Keeping track of the history\n",
    "\n",
    "There are other ways to get more information and description of your model, which are useful when you introduce more complexity to the model and would like to keep track of the changes. We'll summarise them in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, here is how you can return the information on the networks' layers and their types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x21c3b48aac0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x21c3b865070>]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "nonlinear_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how you can get a concise summary of the network layers (note that the first dimension in the output shape column is specified as `None` â€“ this is to denote that this dimension is variable as it depends on the batch size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                30        \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 22        \n=================================================================\nTotal params: 52\nTrainable params: 52\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also plot tje model summary like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(nonlinear_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a number of ways to extract (and store) the information on individual layers, as well as on weights and biases in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dense_1'"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "hidden1 = nonlinear_model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "nonlinear_model.get_layer(hidden1.name) is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.05231333, -0.39751104],\n",
       "       [-0.25937995, -0.32394454],\n",
       "       [ 0.24207664, -0.12807637],\n",
       "       [-0.508541  ,  0.07051337],\n",
       "       [ 0.18466252, -0.5636332 ],\n",
       "       [-0.37810847, -0.10768253],\n",
       "       [ 0.49849516, -0.4668952 ],\n",
       "       [ 0.13537127, -0.24099332],\n",
       "       [ 0.28385347,  0.1845336 ],\n",
       "       [ 0.4535963 , -0.26293108]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and track the changes in the loss and accuracy on the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                       metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.3892 - accuracy: 0.5000 - val_loss: 0.8255 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7854 - accuracy: 0.5000 - val_loss: 0.6642 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9605 - accuracy: 0.5000 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6833 - accuracy: 0.5000 - val_loss: 0.6474 - val_accuracy: 0.7500\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6500 - accuracy: 0.7500 - val_loss: 0.6200 - val_accuracy: 0.7500\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 0.7500 - val_loss: 0.6031 - val_accuracy: 0.7500\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.9004 - accuracy: 0.5000 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6429 - accuracy: 0.5000 - val_loss: 0.5728 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5882 - accuracy: 0.5000 - val_loss: 0.5589 - val_accuracy: 0.7500\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8335 - accuracy: 0.2500 - val_loss: 0.6275 - val_accuracy: 0.7500\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6783 - accuracy: 0.7500 - val_loss: 0.6155 - val_accuracy: 0.7500\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6165 - accuracy: 0.7500 - val_loss: 0.5454 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8130 - accuracy: 0.7500 - val_loss: 0.5998 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8086 - accuracy: 0.5000 - val_loss: 0.7815 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7499 - accuracy: 0.5000 - val_loss: 0.6997 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0031 - accuracy: 0.5000 - val_loss: 0.7196 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7098 - accuracy: 0.5000 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0053 - accuracy: 0.0000e+00 - val_loss: 0.7206 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7105 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.0012 - accuracy: 0.5000 - val_loss: 0.7186 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7092 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0052 - accuracy: 0.0000e+00 - val_loss: 0.7206 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7105 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9962 - accuracy: 0.5000 - val_loss: 0.7156 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7073 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9968 - accuracy: 0.5000 - val_loss: 0.7160 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0447 - accuracy: 0.0000e+00 - val_loss: 0.7361 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7203 - accuracy: 0.5000 - val_loss: 0.6961 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.0059 - accuracy: 0.0000e+00 - val_loss: 0.7209 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7107 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9962 - accuracy: 0.5000 - val_loss: 0.7156 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7073 - accuracy: 0.5000 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.0032 - accuracy: 0.0000e+00 - val_loss: 0.7196 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = nonlinear_model.fit(xor_input, xor_output, batch_size=2, epochs=50,\n",
    "                    validation_data=(xor_input, xor_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 50, 'steps': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    }
   ],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the changes across all epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"306.677344pt\" version=\"1.1\" viewBox=\"0 0 483.703125 306.677344\" width=\"483.703125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-11-24T00:44:46.611088</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 306.677344 \r\nL 483.703125 306.677344 \r\nL 483.703125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 282.799219 \r\nL 476.503125 282.799219 \r\nL 476.503125 10.999219 \r\nL 30.103125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 50.394034 282.799219 \r\nL 50.394034 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m029be64291\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.394034\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(47.212784 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 133.214071 282.799219 \r\nL 133.214071 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"133.214071\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(126.851571 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 216.034108 282.799219 \r\nL 216.034108 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"216.034108\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(209.671608 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 298.854145 282.799219 \r\nL 298.854145 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.854145\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(292.491645 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 381.674183 282.799219 \r\nL 381.674183 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"381.674183\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(375.311683 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 464.49422 282.799219 \r\nL 464.49422 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"464.49422\" xlink:href=\"#m029be64291\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(458.13172 297.397656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 282.799219 \r\nL 476.503125 282.799219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m3c36667423\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"282.799219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 286.598437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 228.439219 \r\nL 476.503125 228.439219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 174.079219 \r\nL 476.503125 174.079219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"174.079219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 177.878437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 119.719219 \r\nL 476.503125 119.719219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"119.719219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 123.518437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 65.359219 \r\nL 476.503125 65.359219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"65.359219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 69.158437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#pd432f4b45a)\" d=\"M 30.103125 10.999219 \r\nL 476.503125 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3c36667423\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_25\">\r\n    <path clip-path=\"url(#pd432f4b45a)\" d=\"M 55.12679 -1 \r\nL 58.676038 69.324703 \r\nL 66.958042 21.742625 \r\nL 75.240045 97.071453 \r\nL 83.522049 106.140242 \r\nL 91.804053 113.6675 \r\nL 100.086056 38.060637 \r\nL 108.36806 108.054643 \r\nL 116.650064 122.9282 \r\nL 124.932067 56.247917 \r\nL 133.214071 98.447187 \r\nL 141.496075 115.226057 \r\nL 149.778079 61.838044 \r\nL 158.060082 63.021153 \r\nL 166.342086 78.977327 \r\nL 174.62409 93.287898 \r\nL 182.906093 94.331439 \r\nL 191.188097 94.397408 \r\nL 199.470101 94.401555 \r\nL 207.752105 10.144673 \r\nL 216.034108 89.874622 \r\nL 224.316112 94.106106 \r\nL 232.598116 9.570687 \r\nL 240.880119 89.692203 \r\nL 249.162123 94.093664 \r\nL 257.444127 10.660854 \r\nL 265.726131 90.047336 \r\nL 274.008134 94.117868 \r\nL 282.290138 9.582255 \r\nL 290.572142 89.695784 \r\nL 298.854145 12.027354 \r\nL 307.136149 90.564117 \r\nL 315.418153 11.863939 \r\nL 323.607151 -1 \r\nM 323.713882 -1 \r\nL 331.98216 87.014432 \r\nL 340.264164 93.906046 \r\nL 348.546168 9.398864 \r\nL 356.828171 89.639244 \r\nL 365.110175 12.037301 \r\nL 373.392179 90.568329 \r\nL 381.674183 94.153088 \r\nL 389.956186 94.38623 \r\nL 398.23819 94.400843 \r\nL 406.520194 94.401766 \r\nL 414.802197 94.401815 \r\nL 423.084201 94.401815 \r\nL 431.366205 94.401815 \r\nL 439.648208 94.401815 \r\nL 447.930212 94.401815 \r\nL 456.212216 10.128213 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_26\">\r\n    <path clip-path=\"url(#pd432f4b45a)\" d=\"M 50.394034 146.899219 \r\nL 58.676038 146.899219 \r\nL 66.958042 146.899219 \r\nL 75.240045 146.899219 \r\nL 83.522049 78.949219 \r\nL 91.804053 78.949219 \r\nL 100.086056 146.899219 \r\nL 108.36806 146.899219 \r\nL 116.650064 146.899219 \r\nL 124.932067 214.849219 \r\nL 133.214071 78.949219 \r\nL 141.496075 78.949219 \r\nL 149.778079 78.949219 \r\nL 158.060082 146.899219 \r\nL 166.342086 146.899219 \r\nL 174.62409 146.899219 \r\nL 182.906093 146.899219 \r\nL 191.188097 146.899219 \r\nL 199.470101 146.899219 \r\nL 207.752105 146.899219 \r\nL 216.034108 146.899219 \r\nL 224.316112 146.899219 \r\nL 232.598116 282.799219 \r\nL 240.880119 146.899219 \r\nL 249.162123 146.899219 \r\nL 257.444127 146.899219 \r\nL 265.726131 146.899219 \r\nL 274.008134 146.899219 \r\nL 282.290138 282.799219 \r\nL 290.572142 146.899219 \r\nL 298.854145 146.899219 \r\nL 307.136149 146.899219 \r\nL 315.418153 146.899219 \r\nL 323.700157 282.799219 \r\nL 331.98216 146.899219 \r\nL 340.264164 146.899219 \r\nL 348.546168 282.799219 \r\nL 356.828171 146.899219 \r\nL 365.110175 146.899219 \r\nL 373.392179 146.899219 \r\nL 381.674183 146.899219 \r\nL 389.956186 146.899219 \r\nL 398.23819 146.899219 \r\nL 406.520194 146.899219 \r\nL 414.802197 146.899219 \r\nL 423.084201 146.899219 \r\nL 431.366205 146.899219 \r\nL 439.648208 146.899219 \r\nL 447.930212 146.899219 \r\nL 456.212216 282.799219 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_27\">\r\n    <path clip-path=\"url(#pd432f4b45a)\" d=\"M 50.394034 58.434569 \r\nL 58.676038 102.281629 \r\nL 66.958042 94.910495 \r\nL 75.240045 106.827485 \r\nL 83.522049 114.277353 \r\nL 91.804053 118.87497 \r\nL 100.086056 95.681965 \r\nL 108.36806 127.110969 \r\nL 116.650064 130.888579 \r\nL 124.932067 112.239584 \r\nL 133.214071 115.508464 \r\nL 141.496075 134.56547 \r\nL 149.778079 119.767214 \r\nL 158.060082 70.394393 \r\nL 166.342086 92.623222 \r\nL 174.62409 94.289205 \r\nL 182.906093 94.394784 \r\nL 191.188097 94.401377 \r\nL 199.470101 94.401798 \r\nL 207.752105 87.218462 \r\nL 216.034108 93.928953 \r\nL 224.316112 94.372168 \r\nL 232.598116 86.931501 \r\nL 240.880119 93.909059 \r\nL 249.162123 94.370904 \r\nL 257.444127 87.490323 \r\nL 265.726131 93.947746 \r\nL 274.008134 94.37335 \r\nL 282.290138 86.937123 \r\nL 290.572142 93.909448 \r\nL 298.854145 88.304838 \r\nL 307.136149 94.003589 \r\nL 315.418153 88.197493 \r\nL 323.700157 82.740551 \r\nL 331.98216 93.609316 \r\nL 340.264164 94.352014 \r\nL 348.546168 86.848231 \r\nL 356.828171 93.903259 \r\nL 365.110175 88.31148 \r\nL 373.392179 94.004026 \r\nL 381.674183 94.376882 \r\nL 389.956186 94.400276 \r\nL 398.23819 94.401717 \r\nL 406.520194 94.401798 \r\nL 414.802197 94.401815 \r\nL 423.084201 94.401815 \r\nL 431.366205 94.401815 \r\nL 439.648208 94.401815 \r\nL 447.930212 94.401815 \r\nL 456.212216 87.210037 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_28\">\r\n    <path clip-path=\"url(#pd432f4b45a)\" d=\"M 50.394034 146.899219 \r\nL 58.676038 146.899219 \r\nL 66.958042 146.899219 \r\nL 75.240045 78.949219 \r\nL 83.522049 78.949219 \r\nL 91.804053 78.949219 \r\nL 100.086056 146.899219 \r\nL 108.36806 146.899219 \r\nL 116.650064 78.949219 \r\nL 124.932067 78.949219 \r\nL 133.214071 78.949219 \r\nL 141.496075 78.949219 \r\nL 149.778079 146.899219 \r\nL 158.060082 146.899219 \r\nL 166.342086 146.899219 \r\nL 174.62409 146.899219 \r\nL 182.906093 146.899219 \r\nL 191.188097 146.899219 \r\nL 199.470101 146.899219 \r\nL 207.752105 146.899219 \r\nL 216.034108 146.899219 \r\nL 224.316112 146.899219 \r\nL 232.598116 146.899219 \r\nL 240.880119 146.899219 \r\nL 249.162123 146.899219 \r\nL 257.444127 146.899219 \r\nL 265.726131 146.899219 \r\nL 274.008134 146.899219 \r\nL 282.290138 146.899219 \r\nL 290.572142 146.899219 \r\nL 298.854145 146.899219 \r\nL 307.136149 146.899219 \r\nL 315.418153 146.899219 \r\nL 323.700157 146.899219 \r\nL 331.98216 146.899219 \r\nL 340.264164 146.899219 \r\nL 348.546168 146.899219 \r\nL 356.828171 146.899219 \r\nL 365.110175 146.899219 \r\nL 373.392179 146.899219 \r\nL 381.674183 146.899219 \r\nL 389.956186 146.899219 \r\nL 398.23819 146.899219 \r\nL 406.520194 146.899219 \r\nL 414.802197 146.899219 \r\nL 423.084201 146.899219 \r\nL 431.366205 146.899219 \r\nL 439.648208 146.899219 \r\nL 447.930212 146.899219 \r\nL 456.212216 146.899219 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 282.799219 \r\nL 30.103125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 476.503125 282.799219 \r\nL 476.503125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 282.799219 \r\nL 476.503125 282.799219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 10.999219 \r\nL 476.503125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 37.103125 277.799219 \r\nL 134.046875 277.799219 \r\nQ 136.046875 277.799219 136.046875 275.799219 \r\nL 136.046875 217.530469 \r\nQ 136.046875 215.530469 134.046875 215.530469 \r\nL 37.103125 215.530469 \r\nQ 35.103125 215.530469 35.103125 217.530469 \r\nL 35.103125 275.799219 \r\nQ 35.103125 277.799219 37.103125 277.799219 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_29\">\r\n     <path d=\"M 39.103125 223.628906 \r\nL 59.103125 223.628906 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_30\"/>\r\n    <g id=\"text_13\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(67.103125 227.128906)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_31\">\r\n     <path d=\"M 39.103125 238.307031 \r\nL 59.103125 238.307031 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_32\"/>\r\n    <g id=\"text_14\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(67.103125 241.807031)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_33\">\r\n     <path d=\"M 39.103125 252.985156 \r\nL 59.103125 252.985156 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_34\"/>\r\n    <g id=\"text_15\">\r\n     <!-- val_loss -->\r\n     <g transform=\"translate(67.103125 256.485156)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_35\">\r\n     <path d=\"M 39.103125 267.941406 \r\nL 59.103125 267.941406 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_36\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_accuracy -->\r\n     <g transform=\"translate(67.103125 271.441406)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd432f4b45a\">\r\n   <rect height=\"271.8\" width=\"446.4\" x=\"30.103125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case of regression\n",
    "\n",
    "Finally, you can address not only classification but also regression tasks with `TensorFlow`. Let's look at a short example here.\n",
    "\n",
    "In Practical 1 you have been using a custom version of the California housing dataset (refer to the Practical 1 to see what differences were introduced in the original data). The original version is [accessible via `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), and the code below shows how to access a dataset from `sklearn` (in fact, `sklearn` provides access to a number of useful ML datasets, so take a look at the [documentation](https://scikit-learn.org/stable/datasets/index.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let't split the dataset into training, validation, and test sets. Note that you can access the data from the dataset with `housing.data`, and the labels with `housing.target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And scale the data using standardisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows to you how to implement a regression model using `TensorFlow`. It is quite similar to the code for classification with minor difference: the loss function that you use here is mean squared error, and the output is a single predicted value thus the dimensionality of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "  1/363 [..............................] - ETA: 0s - loss: 3.7219WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "363/363 [==============================] - 0s 866us/step - loss: 1.6419 - val_loss: 0.8560\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.7047 - val_loss: 0.6531\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.6345 - val_loss: 0.6099\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.5977 - val_loss: 0.5658\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.5706 - val_loss: 0.5355\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.5472 - val_loss: 0.5173\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.5288 - val_loss: 0.5081\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 596us/step - loss: 0.5130 - val_loss: 0.4799\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 632us/step - loss: 0.4992 - val_loss: 0.4690\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 641us/step - loss: 0.4875 - val_loss: 0.4656\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.4777 - val_loss: 0.4482\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4688 - val_loss: 0.4479\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 654us/step - loss: 0.4615 - val_loss: 0.4296\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.4547 - val_loss: 0.4233\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.4488 - val_loss: 0.4176\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.4435 - val_loss: 0.4123\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.4389 - val_loss: 0.4071\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.4347 - val_loss: 0.4037\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.4306 - val_loss: 0.4000\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4273 - val_loss: 0.3969\n",
      "  1/162 [..............................] - ETA: 0s - loss: 0.4649WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "162/162 [==============================] - 0s 491us/step - loss: 0.4212\n"
     ]
    }
   ],
   "source": [
    "reg_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "reg_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(lr=1e-3))\n",
    "history = reg_model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = reg_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"252.317344pt\" version=\"1.1\" viewBox=\"0 0 372.103125 252.317344\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-11-24T00:44:53.141063</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 252.317344 \r\nL 372.103125 252.317344 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 228.439219 \r\nL 364.903125 228.439219 \r\nL 364.903125 10.999219 \r\nL 30.103125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 45.321307 228.439219 \r\nL 45.321307 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mbb9c222f9e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(37.369744 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 85.369154 228.439219 \r\nL 85.369154 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"85.369154\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(77.417591 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 125.417001 228.439219 \r\nL 125.417001 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"125.417001\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 5.0 -->\r\n      <g transform=\"translate(117.465438 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 165.464847 228.439219 \r\nL 165.464847 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.464847\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 7.5 -->\r\n      <g transform=\"translate(157.513285 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 205.512694 228.439219 \r\nL 205.512694 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"205.512694\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 10.0 -->\r\n      <g transform=\"translate(194.379882 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 245.560541 228.439219 \r\nL 245.560541 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"245.560541\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 12.5 -->\r\n      <g transform=\"translate(234.427729 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 285.608388 228.439219 \r\nL 285.608388 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.608388\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 15.0 -->\r\n      <g transform=\"translate(274.475576 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 325.656235 228.439219 \r\nL 325.656235 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.656235\" xlink:href=\"#mbb9c222f9e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 17.5 -->\r\n      <g transform=\"translate(314.523423 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 228.439219 \r\nL 364.903125 228.439219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m462949eb5c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 184.951219 \r\nL 364.903125 184.951219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 141.463219 \r\nL 364.903125 141.463219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 145.262437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 97.975219 \r\nL 364.903125 97.975219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 101.774437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_25\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 54.487219 \r\nL 364.903125 54.487219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 58.286437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_27\">\r\n      <path clip-path=\"url(#p5d481efbc8)\" d=\"M 30.103125 10.999219 \r\nL 364.903125 10.999219 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_28\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m462949eb5c\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_29\">\r\n    <path clip-path=\"url(#p5d481efbc8)\" d=\"M 55.349557 -1 \r\nL 61.340446 75.210017 \r\nL 77.359584 90.464396 \r\nL 93.378723 98.467425 \r\nL 109.397862 104.361662 \r\nL 125.417001 109.446105 \r\nL 141.436139 113.466211 \r\nL 157.455278 116.889852 \r\nL 173.474417 119.902233 \r\nL 189.493556 122.437385 \r\nL 205.512694 124.560699 \r\nL 221.531833 126.511458 \r\nL 237.550972 128.08849 \r\nL 253.570111 129.565442 \r\nL 269.589249 130.858498 \r\nL 285.608388 132.010959 \r\nL 301.627527 133.009716 \r\nL 317.646666 133.927477 \r\nL 333.665804 134.805741 \r\nL 349.684943 135.522148 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_30\">\r\n    <path clip-path=\"url(#p5d481efbc8)\" d=\"M 45.321307 42.321024 \r\nL 61.340446 86.434323 \r\nL 77.359584 95.823069 \r\nL 93.378723 105.402436 \r\nL 109.397862 112.008758 \r\nL 125.417001 115.95626 \r\nL 141.436139 117.956289 \r\nL 157.455278 124.08498 \r\nL 173.474417 126.469317 \r\nL 189.493556 127.191278 \r\nL 205.512694 130.974513 \r\nL 221.531833 131.044746 \r\nL 237.550972 135.032276 \r\nL 253.570111 136.396906 \r\nL 269.589249 137.64692 \r\nL 285.608388 138.789914 \r\nL 301.627527 139.908581 \r\nL 317.646666 140.663824 \r\nL 333.665804 141.456763 \r\nL 349.684943 142.129364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 228.439219 \r\nL 30.103125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 228.439219 \r\nL 364.903125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 228.439219 \r\nL 364.903125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 10.999219 \r\nL 364.903125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p5d481efbc8\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also explore model's predictions on some selected datapoints and compare them to the true values for these datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.3885664],\n",
       "       [1.6792021],\n",
       "       [3.1022797]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = reg_model.predict(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Classification of House Locations\n",
    "\n",
    "In the first practical, you used the California House Prices Dataset in order to predict the prices of the houses based on various properties about the houses. In this assignment, we will experiment with `TensorFlow` and train a model to predict the \"ocean proximity\" of a house.\n",
    "\n",
    "First, let's read in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('housing/housing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the ocean proximity column from the other features and convert the values to numerical IDs. Remember, the `ocean_proximity` column already contains discrete classes, so it is well-suited for the classification task. However, these are strings and in order to optimise the softmax function in `TensorFlow`, we need numerical IDs instead of strings. We can use the `pandas` map function to do the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy().drop([\"ocean_proximity\"], axis=1)\n",
    "Y = data.copy()[\"ocean_proximity\"]\n",
    "Y = data.copy()[\"ocean_proximity\"].map({\"<1H OCEAN\":0, \"INLAND\":1, \"ISLAND\": 2, \"NEAR BAY\": 3, \"NEAR OCEAN\": 4}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split off some data for development and testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=42)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's preprocess the input features before giving them to the network. We need to fill in missing values with the imputer, and standardise the values to a similar range using the scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn import preprocessing\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train = imputer.transform(X_train)\n",
    "X_dev = imputer.transform(X_dev)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_dev = scaler.transform(X_dev)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that we can work with.\n",
    "\n",
    "Input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(13209, 9)\n(3303, 9)\n(4128, 9)\n[[-0.69155432  1.10281811 -0.12449485 -0.44361185 -0.60289408 -0.48710064\n  -0.64120663  0.44340968 -0.25873131]\n [ 0.8544348  -0.72493883 -1.07770852  1.75575918  1.99734983  1.69902706\n   2.04218568  0.00321001 -0.28999612]\n [ 0.86440892 -0.88428174 -0.20392932 -0.15088981 -0.02963101 -0.13535041\n  -0.16516379 -0.52181236 -0.01729749]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the correstponding gold-standard labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(13209,)\n(3303,)\n(4128,)\n[1 0 0 4 1 1 3 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code examples above, construct a `TensorFlow` model, then train, tune and test it on this dataset. Experiment with different model settings and hyperparameters. Calculate and evaluate classification accuracy - the percentage of datapoints where the predicted class matches the gold-standard class.\n",
    "\n",
    "During the practical session, give examples of what you tried and what your findings were.\n",
    "\n",
    "Some suggestions and tips:\n",
    "\n",
    "- The XOR classification code can be a good place to start.\n",
    "- The output layer needs to have size 5, because the dataset has 5 possible classes.\n",
    "- Try testing on the development set as you are training, to make sure you don't overfit.\n",
    "- Evaluate on the dev set as much as you want, but evaluate on the test set only after you have chosen a good set of hyperparameters.\n",
    "- You could try different learning rates, hidden layer sizes, learning strategies, etc.\n",
    "- Adaptive learning rates can (and sometimes should) be used together with a regular hand-picked learning rate, and different adaptive learning rates can prefer very different regular learning rates.\n",
    "\n",
    "There are a number of additional (optional) steps that you can try: you can visualise your network architecture, changes in loss and metrics, print out and visualise confusion matrices, implement \"traditional\" machine learning algorithms (e.g., from Practicals 2 and 3) and compare the results, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}