{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Practical 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Your tasks\n",
    "\n",
    "1. Run the code in the notebook. During the practical session, be prepared to discuss the methods and answer the questions from this notebook:\n",
    "\n",
    "    * When applied to a new, test instance bagging classifier aggregates the predictions of all predictors and estimates the statistical mode. Is this similar to the hard or the soft voting strategy?\n",
    "    * [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) automatically performs soft voting when the base classifier can estimate class probabilities (i.e., has a `predict_proba()` method). Which strategy – hard or soft voting – does `BaggingClassifier` follow with the `DecisionTreeClassifier` as the base classifier?\n",
    "    * What do the feature importance weights obtained on the *iris* dataset suggest? Are they similar to your observations from the previous experiments on this dataset?\n",
    "    * We said earlier than one of the advantages of bagging and pasting strategies is that predictors can all be trained in parallel. Is the same applicable to AdaBoost?\n",
    "    * One of the hyperparameters of the boosting algorithms is learning_rate. What is it responsible for and what effect on the resulting ensemble does it have?\n",
    "    * What generalisation behaviour can you expect from each of the tree regressor ensembles (using different learning rates and different number of estimators – see above)?\n",
    "\n",
    "2. Apply ensemble techniques of your choice to one of the datasets you've worked on during the previous practicals and report your findings.\n",
    "\n",
    "3. **Optional**: If you want more practice with these techniques, try implementing a [*stacking algorithm*](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking). There is no available implementation of this approach in `sklearn` so it needs to be implemented from scratch. The idea is as follows:\n",
    "    * Split a dataset of your choice into three subsets – training, validation and test.\n",
    "    * Train a number of predictors on the training data and apply them to the validation data.\n",
    "    * Treat the predictions of those predictors on the validation data to generate new training set: you can use the predictions of the estimators as new features (i.e., each training instance will have as many features as the number of predictors you originally used), and the validation data targets as the new training instances targets. \n",
    "    * Now train a *blender* – a classifier of your choice – on the new training data created this way. Together with the original classifiers, the blender forms a stacking ensemble.\n",
    "    * Finally, apply your original classifiers to the test set, feed their predictions to the blender, and use blender's output as the prediction on the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Task 1\n",
    "- When applied to a new, test instance bagging classifier aggregates the predictions of all predictors and estimates the statistical mode. Is this similar to the hard or the soft voting strategy?\n",
    "\n",
    "** Answer ** Hard, since we choose the most frequent prediction.\n",
    "    \n",
    "- [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) automatically performs soft voting when the base classifier can estimate class probabilities (i.e., has a `predict_proba()` method). Which strategy – hard or soft voting – does `BaggingClassifier` follow with the `DecisionTreeClassifier` as the base classifier?\n",
    "\n",
    "**Answer**: DecisionTreeClassifier has predict_proba() method, hence we're dealing with soft voting\n",
    "\n",
    "- What do the feature importance weights obtained on the *iris* dataset suggest? Are they similar to your observations from the previous experiments on this dataset?\n",
    "\n",
    "**Answer**: They suggest that the petal is a more definitory factor in classification.\n",
    "\n",
    "- We said earlier than one of the advantages of bagging and pasting strategies is that predictors can all be trained in parallel. Is the same applicable to AdaBoost?\n",
    "\n",
    "**Answer**: No, since each predictor depends on the previous one, so the training has to be don sequentially.\n",
    "\n",
    "- One of the hyperparameters of the boosting algorithms is learning_rate. What is it responsible for and what effect on the resulting ensemble does it have?\n",
    "\n",
    "**Answer**: The learning rate scales the contribution of each tree: the lower the rate, the more trees we ned to include in the ensemble, with the predictions generalising better. \n",
    "\n",
    "\n",
    "- What generalisation behaviour can you expect from each of the tree regressor ensembles (using different learning rates and different number of estimators – see above)?\n",
    "**Answer**: As we see above, the first regressor with learning_rate = 1 and 3 trees is underfitting the data, but the 2nd one is overfitting it. We see below that the best result is in the middle, with only 68 trees, so I would expect the 2 regressors to perform poorly when exposed to new data in a similar fashion\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Task 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Apply ensemble techniques of your choice to one of the datasets you've worked on during the previous practicals and report your findings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "source": [
    "## Simple voting classifiers: hard and soft voting strategies\n",
    "\n",
    "- a sufficient number of weak learners, and\n",
    "- their decisions are sufficiently diverse."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "list(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X, y = digits[\"data\"], digits[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.2,\n            train_size=None)\nTRAIN: 1437 TEST: 360\n(1437, 64) (1437,) (360, 64) (360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "split.get_n_splits(X, y)\n",
    "print(split)       \n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "source": [
    "The code below shows how to apply several classifiers to this data.The point here is that we are combining *diverse* classifiers in an ensemble: in essence, `RandomForestClassifier` runs recursive partitioning on the data, while `Support Vector Machines` try to construct a linear boundary between the classes (as discussed in Lecture 3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC # Support Vector Machines\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Let's look into how correlated the predictions of the classifiers are: first, get the predictions on the test data, next store them as a `pandas` DataFrame, and finally apply `.corr()` function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    lr  rf  svc\n",
       "0    5   5    5\n",
       "1    2   2    2\n",
       "2    8   8    8\n",
       "3    8   1    1\n",
       "4    7   7    7\n",
       "..  ..  ..  ...\n",
       "95   2   2    2\n",
       "96   7   7    7\n",
       "97   8   8    8\n",
       "98   4   4    4\n",
       "99   2   2    2\n",
       "\n",
       "[100 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr</th>\n      <th>rf</th>\n      <th>svc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_predictions(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test)\n",
    "\n",
    "\n",
    "preds = {'lr': get_predictions(log_clf), \n",
    "        'rf': get_predictions(rnd_clf), \n",
    "        'svc': get_predictions(svm_clf)}\n",
    "df = pd.DataFrame(data=preds)\n",
    "df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           lr        rf       svc\n",
       "lr   1.000000  0.946742  0.947750\n",
       "rf   0.946742  1.000000  0.970502\n",
       "svc  0.947750  0.970502  1.000000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lr</th>\n      <th>rf</th>\n      <th>svc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>lr</th>\n      <td>1.000000</td>\n      <td>0.946742</td>\n      <td>0.947750</td>\n    </tr>\n    <tr>\n      <th>rf</th>\n      <td>0.946742</td>\n      <td>1.000000</td>\n      <td>0.970502</td>\n    </tr>\n    <tr>\n      <th>svc</th>\n      <td>0.947750</td>\n      <td>0.970502</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "source": [
    "### Hard voting\n",
    "Will this correlation in the individual classifiers' predictions be sufficient for an ensemble? Let's check this out by combining the votes with a *hard voting strategy*: the ensemble classifier will simply choose the majority class predicted by the three classifiers.\n",
    "\n",
    "The code below prints out individual classifiers' accuracy scores along with the ensemble's accuracy score:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.9583333333333334\n",
      "RandomForestClassifier 0.9611111111111111\n",
      "SVC 0.9916666666666667\n",
      "VotingClassifier 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "**Observation:** SVC did better than the voting classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Soft voting \n",
    "\n",
    "`SVC` classifier doesn't estimate class probabilities by default, so you need to set the `probability` hyperparameter to `True`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(probability=True, random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "log_clf = LogisticRegression(random_state=42) # estimate class probabilities by default\n",
    "rnd_clf = RandomForestClassifier(random_state=42) # estimate class probabilities by default\n",
    "svm_clf = SVC(probability=True, random_state=42) # estimate class probabilities with True \n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Now let's estimate the accuracy of the voting classifier in this mode:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.9583333333333334\n",
      "RandomForestClassifier 0.9611111111111111\n",
      "SVC 0.9916666666666667\n",
      "VotingClassifier 0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "**Observation**: Despite using a soft voting strategy which is supposed to give more weight to the individual classifiers with higher accuracies, the final soft VotingClassifier performs slightly worse than the hard one"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bagging ensembles: bagging and pasting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's use `sklearn's BaggingClassifier` to train an ensemble of $500$ [`Decision Trees`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), each trained on $100$ training instances randomly selected from the training set with replacement (for bagging, but you can change the strategy to pasting by setting `bootstrap` to `False`). In addition, both bagging and pasting strategies are good for parallelisation: predictors can all be trained in parallel on different CPU cores or even different servers. Setting `n_jobs` parameter to $-1$ tells `sklearn` to use all available CPU cores:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#use bootstrap=False for pasting\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "#n_jobs = use all of the available CPU cores\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "Let's calculate the accuracy score for the bagging classifier that combines $500$ Decision Tree estimators with a prediction of a single Decision Tree trained on the same data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9222222222222223\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.825\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "source": [
    "### Out-of-Bag evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9498956158663883"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "source": [
    "This means that the bagging classifier is likely to achieve around $95\\%$ accuracy on the test data, too. Let's check the results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9305555555555556"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "Close! \n",
    "\n",
    "Take a look into the decision process by printing out the predictions of the classifier on the training instances."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.09042553, 0.07978723, 0.01595745, ..., 0.        , 0.2712766 ,\n",
       "        0.38297872],\n",
       "       [0.98333333, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.01666667],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.41361257, 0.0052356 , 0.0104712 , ..., 0.05235602, 0.06806283,\n",
       "        0.21465969],\n",
       "       [0.00588235, 0.        , 0.        , ..., 0.94705882, 0.01764706,\n",
       "        0.        ],\n",
       "       [0.        , 0.04069767, 0.02325581, ..., 0.02906977, 0.04069767,\n",
       "        0.34302326]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "source": [
    "## Random Forests\n",
    "\n",
    "[`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) is an ensemble of Decision Trees typically trained via the bagging method. The number of training instances (`max_samples`) is usually set to the total size of the training set. So, in fact, `RandomForestClassifier` will be roughly equivalent to the `BaggingClassifier` that takes Decision Trees as base estimators with the following parameters:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "The key difference is that, rather than using `BaggingClassifier` and passing it a `DecisionTreeClassifier`, you can rely on the `sklearn's RandomForestClassifier` implementation, which is more convenient and optimised for Decision Trees:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, \n",
    "                                 n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "Let's estimate the difference between the two classifiers' predictions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9388888888888889"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "np.sum(y_pred == y_pred_rf) / len(y_pred)  # see to what extent predictions are identical"
   ]
  },
  {
   "source": [
    "### Feature importance\n",
    "\n",
    "Another useful property of the Random Forests classifier is that it can help you measure the relative importance of each feature by looking at how much the tree nodes that use this particular feature reduce impurity of the nodes on average (i.e., across all trees in the forest). The results are scaled so that the sum of all feature importances is equal to $1$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"243.45375pt\" version=\"1.1\" viewBox=\"0 0 356.247625 243.45375\" width=\"356.247625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-11-17T10:46:00.507873</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 243.45375 \r\nL 356.247625 243.45375 \r\nL 356.247625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p1ee58e5004)\">\r\n    <image height=\"218\" id=\"image009802425a\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAADmUlEQVR4nO3csQoWBBiF4fNnQ+gWSCIRQmZBtLaJtOTiFTg4uAhJ4djSkDg2pE6Rg5fg7BCBW5CuUSgNLi4tErT9Dd3Du/Q8N3Cml2/7Die34yKnq6Ftf34ejm374XG3deNat7UPuqlfv+m2tu1WuPVGuAX/W0KDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDwKF8Cf5hNbTtUbi1be992m09+aXbuvhVt7Wz4da2c193Wy4aBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBIP29f78a2nb9+Fm4tm3nu6lPfuy2XndTezvc2nbqWbflokFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBBIX4L//X21tO1FuLXt93vd1oXjk27sxMVu61o3tW2nHnZbLhoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAaBN8ux3251Wx8dv+zGtl3453439nP4pvtSN7X3w62YiwYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUEgfQn+INz67nT4onvbroRbf4RbPx2zqTuHQ7a1bSfCLRcNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgkP7ev1yOXS3Hti/udVs3H3ZbH5/t/uE/zZb+81a45aJBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQOJzcjtXYmWpo2/N3w7Ftd192W391U/v2nW7r7qtua9tuh1suGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoF/AX2nMR96oiCvAAAAAElFTkSuQmCC\" y=\"-11.199062\"/>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_2\">\r\n    <path clip-path=\"url(#p328b7b529a)\" d=\"M 241.38 229.199062 \r\nL 241.38 228.349687 \r\nL 241.38 12.608437 \r\nL 241.38 11.759062 \r\nL 252.252 11.759062 \r\nL 252.252 12.608437 \r\nL 252.252 228.349687 \r\nL 252.252 229.199062 \r\nz\r\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\r\n   </g>\r\n   <image height=\"218\" id=\"image84ce850b79\" transform=\"scale(1 -1)translate(0 -218)\" width=\"11\" x=\"241\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAAsAAADaCAYAAABwzrisAAAA3klEQVR4nO1ZSQrDMBCT2+kC/f9re5jeio9WYIxkxmchtBiFJOMDJBZPPFaRAOLGgO9l4DoZVBoimo836Fi3iEGK+VnGLBLd8XXXXX4RzY5TIBKdo0HHukUM9vLP4OPr7uW/zGw4jCOZVzwQDgOEjgDhkGSuk1GXhkh0XfcMJp5tlQZFonO8G451i0TnWHcv/2XmrnsGH1933d0Q0ey4/CLROU6BY90i0YnUPdapA4gy8FtDBgWmonM02HXvAb80ZHTde8AiafTy7wEfb7C37n9G5nf5O//IzGUw82sCP4OxEToKYUcoAAAAAElFTkSuQmCC\" y=\"-11\"/>\r\n   <g id=\"matplotlib.axis_1\"/>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 3.5 0 \r\n\" id=\"mb620bb9e55\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.252\" xlink:href=\"#mb620bb9e55\" y=\"229.199062\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- Not important -->\r\n      <g transform=\"translate(259.252 233.758125)scale(0.12 -0.12)\">\r\n       <defs>\r\n        <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n        <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n        <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n        <path id=\"DejaVuSans-32\"/>\r\n        <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n        <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n        <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n        <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n        <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n        <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-78\"/>\r\n       <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"135.986328\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"175.195312\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"206.982422\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"234.765625\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"332.177734\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"395.654297\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"456.835938\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"497.949219\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"537.158203\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"598.4375\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"661.816406\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.252\" xlink:href=\"#mb620bb9e55\" y=\"11.759062\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- Very important -->\r\n      <g transform=\"translate(259.252 16.318125)scale(0.12 -0.12)\">\r\n       <defs>\r\n        <path d=\"M 28.609375 0 \r\nL 0.78125 72.90625 \r\nL 11.078125 72.90625 \r\nL 34.1875 11.53125 \r\nL 57.328125 72.90625 \r\nL 67.578125 72.90625 \r\nL 39.796875 0 \r\nz\r\n\" id=\"DejaVuSans-86\"/>\r\n        <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n        <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-86\"/>\r\n       <use x=\"60.658203\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"122.181641\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"163.294922\" xlink:href=\"#DejaVuSans-121\"/>\r\n       <use x=\"222.474609\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"254.261719\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"282.044922\" xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"379.457031\" xlink:href=\"#DejaVuSans-112\"/>\r\n       <use x=\"442.933594\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"504.115234\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"545.228516\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"584.4375\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"645.716797\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"709.095703\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 241.38 229.199062 \r\nL 241.38 228.349687 \r\nL 241.38 12.608437 \r\nL 241.38 11.759062 \r\nL 252.252 11.759062 \r\nL 252.252 12.608437 \r\nL 252.252 228.349687 \r\nL 252.252 229.199062 \r\nz\r\n\" style=\"fill:none;stroke:#000000;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p1ee58e5004\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"11.759062\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p328b7b529a\">\r\n   <rect height=\"217.44\" width=\"10.872\" x=\"241.38\" y=\"11.759062\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "rnd_clf.fit(digits[\"data\"], digits[\"target\"])\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(8, 8)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plot_digit(rnd_clf.feature_importances_)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## AdaBoost\n",
    "\n",
    "*Boosting*, or *hypothesis boosting* is an ensemble method that can combine several weak learners into a strong learner. The general idea is to train predictors sequentially in such a way that each next predictor tries to correct its predecessor. The most popular among boosting methods are *AdaBoost* (which stands for *Adaptive Boosting*) and *Gradient Boosting*. Let's apply *AdaBoost* first.\n",
    "\n",
    "The idea behind *AdaBoost* is as follows: in order to correct the predecessor's mistakes, the algorithm assigns higher weights to the training instances that the predecessor underfitted. I.e., the next step pays more attention to such instances and the algorithm focuses more on the hard cases.\n",
    "\n",
    "Here is the step-by-step strategy:\n",
    "- start with the first classifier (i.e., this can be a Decision Trees classifier)\n",
    "- train it and use it to make predictions on the training set\n",
    "- increase the relative weight of misclassified training instances\n",
    "- train a second classifier with these new updated weights and make new predictions\n",
    "- update weights using the new predictions\n",
    "- continue until stopping criteria are satisfied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Here is the full list of parameters and attributes of the algorithm:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['base_estimator_',\n",
       " 'classes_',\n",
       " 'estimator_errors_',\n",
       " 'estimator_weights_',\n",
       " 'estimators_',\n",
       " 'feature_importances_',\n",
       " 'n_classes_',\n",
       " 'n_features_in_']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "list(m for m in dir(ada_clf) if not m.startswith(\"_\") and m.endswith(\"_\"))"
   ]
  },
  {
   "source": [
    "E.g., you can check classification errors for each estimator in the boosted ensemble as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.80097425, 0.77877399, 0.76556079, 0.76784349, 0.79536663,\n",
       "       0.75803658, 0.75632757, 0.78008056, 0.78725409, 0.75427787,\n",
       "       0.77933258, 0.77230468, 0.80442323, 0.78589353, 0.78081672,\n",
       "       0.79615547, 0.80218109, 0.81377651, 0.79147006, 0.74990234,\n",
       "       0.81283776, 0.78063383, 0.77181485, 0.75637328, 0.80925024,\n",
       "       0.76877274, 0.78305977, 0.78316466, 0.80707849, 0.79110479,\n",
       "       0.79390387, 0.76385776, 0.75897325, 0.8183144 , 0.79292414,\n",
       "       0.7823857 , 0.75721273, 0.81507925, 0.76643509, 0.78479321,\n",
       "       0.77038703, 0.82162682, 0.77058078, 0.75786408, 0.75900907,\n",
       "       0.80816434, 0.77250513, 0.78096649, 0.78050773, 0.82345498,\n",
       "       0.78252747, 0.76422285, 0.76184596, 0.81559969, 0.77168095,\n",
       "       0.78831008, 0.78137069, 0.80636622, 0.80652607, 0.77803365,\n",
       "       0.7811047 , 0.74199013, 0.77271518, 0.76447835, 0.75185125,\n",
       "       0.76041115, 0.77187425, 0.79060624, 0.76522733, 0.75418758,\n",
       "       0.74838979, 0.71661483, 0.74982378, 0.73616979, 0.69118601,\n",
       "       0.69037582, 0.73847137, 0.77308327, 0.79558335, 0.774358  ,\n",
       "       0.75325414, 0.74027117, 0.751257  , 0.76577981, 0.7413875 ,\n",
       "       0.75701104, 0.76085828, 0.76200728, 0.78335888, 0.78196297,\n",
       "       0.76069254, 0.75004133, 0.748129  , 0.74615609, 0.77334451,\n",
       "       0.74614951, 0.73019251, 0.71010616, 0.76119381, 0.74840263,\n",
       "       0.76536992, 0.76413279, 0.74565899, 0.70765657, 0.74441699,\n",
       "       0.7314149 , 0.74144389, 0.76415212, 0.76873563, 0.76067747,\n",
       "       0.78576685, 0.77845199, 0.76686827, 0.74808999, 0.74667576,\n",
       "       0.76347501, 0.79175649, 0.76264777, 0.74178799, 0.72459277,\n",
       "       0.71778945, 0.76400186, 0.76634866, 0.76229385, 0.76022352,\n",
       "       0.74064424, 0.76322778, 0.76760104, 0.77352299, 0.7535272 ,\n",
       "       0.74377192, 0.73716513, 0.75863782, 0.74646183, 0.71929862,\n",
       "       0.75404875, 0.78129478, 0.77120155, 0.7719121 , 0.79102949,\n",
       "       0.75200497, 0.76061245, 0.78362574, 0.78514419, 0.75076199,\n",
       "       0.72789749, 0.67306396, 0.74188904, 0.75819537, 0.75739618,\n",
       "       0.75249895, 0.75547766, 0.76210767, 0.78659669, 0.80914553,\n",
       "       0.77628598, 0.78212987, 0.76204641, 0.78788262, 0.78129026,\n",
       "       0.77691561, 0.78811206, 0.77008088, 0.7286982 , 0.73076005,\n",
       "       0.74201998, 0.74400369, 0.73404403, 0.61992871, 0.69436421,\n",
       "       0.72429586, 0.78516295, 0.75941634, 0.70156677, 0.67742194,\n",
       "       0.68369913, 0.7443166 , 0.72891611, 0.76539633, 0.76244844,\n",
       "       0.75080509, 0.73669949, 0.73795394, 0.7064007 , 0.71697938,\n",
       "       0.75659721, 0.72951446, 0.76876555, 0.76055482, 0.78384046,\n",
       "       0.75257096, 0.76066995, 0.73481485, 0.71962915, 0.78244933,\n",
       "       0.79139251, 0.75832041, 0.69214648, 0.68922158, 0.69761889])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "ada_clf.estimator_errors_"
   ]
  },
  {
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting is another popular boosting algorithm. It also works sequentially adding new predictors to the ensemble, each one correcting the errors from its predecessor. Unlike AdaBoost, it's trying to fit each new predictor to the *residual errors* made by the previous predictor. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Next, let's fit a single [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Now train a second `DecisionTreeRegressor` on the residual errors made by the first predictor:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X_train, y2)"
   ]
  },
  {
   "source": [
    "Finally, let's train a third regressor on the residual errors made by the second predictor:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Finally, let's train a third regressor on the residual errors made by the second predictor:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X_train, y3)"
   ]
  },
  {
   "source": [
    "Just built an ensemble containing three trees. It can now be applied to new instances by making predictions based on adding up the predictions from all three trees:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 8.28764437,  5.04560255,  6.30426495,  7.34461479,  5.76629421,\n",
       "        2.0144583 ,  5.04560255,  3.38732798,  6.08595238,  6.30426495,\n",
       "        0.70006663,  5.24335295,  9.54630677,  4.7130827 ,  5.04560255,\n",
       "        5.24335295,  4.50763181,  3.66503238,  8.48539477,  6.91477469,\n",
       "        4.50763181,  2.76828665,  8.28764437,  2.76828665,  3.05480814,\n",
       "        5.24335295,  5.04560255,  0.70006663,  5.25650013,  3.58507838,\n",
       "        6.08595238,  9.54630677,  4.50763181,  6.08595238,  4.50763181,\n",
       "        4.50763181,  5.24335295,  6.08595238,  0.70006663,  8.28764437,\n",
       "        2.76828665,  3.80863649,  6.91477469,  4.50763181,  6.30426495,\n",
       "        5.24335295,  5.24335295,  3.46728198,  9.54630677,  5.24335295,\n",
       "        4.50763181,  1.50962425,  5.24335295,  5.24335295,  5.04560255,\n",
       "        3.38732798,  2.0806368 ,  0.70006663,  4.50763181, -0.87055345,\n",
       "        7.9679862 ,  2.2783872 ,  6.08595238,  3.80863649,  4.7130827 ,\n",
       "        9.54630677,  5.04560255,  6.08595238,  4.72594438,  4.50763181,\n",
       "        8.28764437,  0.50231623,  4.64599038,  6.08595238,  3.38732798,\n",
       "        6.08595238,  6.30426495,  2.0144583 ,  2.0144583 ,  5.24335295,\n",
       "        5.04560255,  6.08595238,  3.46728198,  5.76629421,  8.28764437,\n",
       "        5.04560255,  5.04560255,  3.46728198,  0.70006663,  6.30426495,\n",
       "        3.46728198,  8.28764437,  2.76828665,  5.68634022,  5.04560255,\n",
       "        3.46728198,  5.68634022,  6.30426495,  3.46728198,  2.0944123 ,\n",
       "        4.50763181,  5.04560255,  2.0144583 ,  6.30426495,  4.7130827 ,\n",
       "        8.28764437,  6.08595238,  3.46728198,  1.50962425,  4.50763181,\n",
       "        5.04560255,  6.08595238,  2.76828665,  4.50763181,  6.08595238,\n",
       "        5.24335295,  5.04560255,  7.34461479,  5.04560255,  4.50763181,\n",
       "        0.50231623,  8.28764437,  4.50763181,  4.50763181,  4.7130827 ,\n",
       "        2.0144583 ,  3.67273287,  2.76828665,  4.50763181,  6.08595238,\n",
       "        2.0806368 ,  4.42767782,  6.08595238,  3.66503238,  1.1771044 ,\n",
       "        8.28764437,  2.0144583 ,  4.64599038,  6.08595238,  3.13476213,\n",
       "        6.30426495,  5.68634022,  0.50231623,  4.50763181,  5.04560255,\n",
       "       -0.1966791 ,  4.50763181,  6.30426495,  3.67273287,  5.24335295,\n",
       "        4.70538221,  6.08595238,  4.50763181,  4.50763181,  6.08595238,\n",
       "        1.50962425,  9.54630677,  4.72594438,  6.08595238,  5.68634022,\n",
       "        9.54630677,  3.38732798,  5.76629421,  3.67273287,  4.50763181,\n",
       "        5.04560255,  8.28764437,  0.50231623,  4.64599038,  4.42767782,\n",
       "        0.50231623,  8.28764437,  6.30426495,  5.04560255,  8.28764437,\n",
       "        6.30426495,  3.3392992 ,  5.04560255,  6.30426495,  6.08595238,\n",
       "        6.30426495,  5.76629421,  6.30426495,  1.50962425,  6.08595238,\n",
       "        5.24335295,  3.66503238,  5.68634022,  3.05480814,  0.50231623,\n",
       "        4.70538221,  8.28764437,  5.68634022,  7.9679862 ,  5.24335295,\n",
       "        5.04560255,  5.24335295,  6.30426495,  4.72594438,  5.04560255,\n",
       "        5.04560255,  3.46728198,  6.08595238,  3.38732798,  6.30426495,\n",
       "        5.24335295,  2.0944123 ,  2.76828665,  2.0144583 ,  6.30426495,\n",
       "        6.08595238,  6.7093238 ,  5.33645412,  6.08595238,  6.7093238 ,\n",
       "        4.70538221,  2.76828665,  5.24335295,  1.76097863,  5.24335295,\n",
       "        5.24335295,  4.50763181,  5.04560255,  5.68634022,  6.30426495,\n",
       "        3.67273287,  5.24335295,  3.66503238,  6.28370279,  0.70006663,\n",
       "        3.66503238,  4.64599038,  3.05480814,  4.64599038,  2.76828665,\n",
       "        3.3392992 ,  5.24335295,  2.0944123 ,  4.72594438,  6.9070742 ,\n",
       "        5.24335295,  0.50231623,  2.0944123 ,  6.08595238,  7.9679862 ,\n",
       "        0.70006663,  3.66503238,  6.30426495,  2.0944123 ,  3.67273287,\n",
       "        8.28764437,  4.50763181,  3.46728198,  4.50763181,  6.08595238,\n",
       "        3.38732798,  3.13476213,  8.28764437,  3.67273287,  2.0944123 ,\n",
       "        0.50231623,  4.50763181,  6.08595238,  7.34461479,  1.50962425,\n",
       "        5.04560255,  3.66503238,  0.13675457,  5.04560255,  4.50763181,\n",
       "        2.0944123 ,  6.08595238,  1.50962425,  4.72594438,  5.04560255,\n",
       "        9.54630677,  3.66503238,  5.04560255,  5.04560255,  0.70006663,\n",
       "        5.04560255,  3.66503238,  6.30426495,  5.04560255,  9.54630677,\n",
       "        8.28764437,  5.04560255,  2.0944123 ,  5.76629421,  5.04560255,\n",
       "        6.08595238,  5.04560255,  4.72594438,  5.76629421,  5.04560255,\n",
       "        8.28764437,  6.30426495,  2.0144583 ,  0.50231623,  6.08595238,\n",
       "        4.72594438,  1.50962425,  2.0944123 ,  5.04560255,  3.38732798,\n",
       "        2.2783872 ,  5.04560255,  7.88803221,  3.66503238,  1.50962425,\n",
       "        6.30426495,  2.76828665,  5.24335295,  4.50763181,  5.24335295,\n",
       "        6.08595238,  2.76828665,  3.38732798,  3.38732798,  5.04560255,\n",
       "        4.70538221,  1.50962425,  3.38732798,  4.50763181,  5.04560255,\n",
       "        3.58507838,  3.66503238,  6.30426495,  6.30426495,  9.54630677,\n",
       "        5.24335295,  0.70006663,  4.7130827 ,  6.30426495,  3.67273287,\n",
       "        4.72594438,  2.0144583 ,  7.88803221,  7.9679862 ,  6.30426495,\n",
       "        7.9679862 ,  3.46728198,  4.72594438,  2.76828665,  1.50962425,\n",
       "        8.28764437,  6.30426495,  5.04560255,  5.04560255,  6.08595238,\n",
       "        4.72594438,  4.50763181,  6.30426495,  6.30426495,  8.28764437,\n",
       "        5.24335295,  3.67273287,  0.70006663,  2.0144583 ,  6.30426495])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "A simpler way to train ensembles of *Gradient Boosted Regression Trees* is to use [`sklearn's GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) implementation. In addition to the typical hyperparameters that control the growth of the trees, it also includes hyperparameters that control the ensemble training, e.g. `n_estimators`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 8.28764437,  5.04560255,  6.30426495,  7.34461479,  5.76629421,\n",
       "        2.0144583 ,  5.04560255,  3.38732798,  6.08595238,  6.30426495,\n",
       "        0.70006663,  5.24335295,  9.54630677,  4.7130827 ,  5.04560255,\n",
       "        5.24335295,  4.50763181,  3.66503238,  8.48539477,  6.91477469,\n",
       "        4.50763181,  2.76828665,  8.28764437,  2.76828665,  3.05480814,\n",
       "        5.24335295,  5.04560255,  0.70006663,  5.25650013,  3.58507838,\n",
       "        6.08595238,  9.54630677,  4.50763181,  6.08595238,  4.50763181,\n",
       "        4.50763181,  5.24335295,  6.08595238,  0.70006663,  8.28764437,\n",
       "        2.76828665,  3.80863649,  6.91477469,  4.50763181,  6.30426495,\n",
       "        5.24335295,  5.24335295,  3.46728198,  9.54630677,  5.24335295,\n",
       "        4.50763181,  1.50962425,  5.24335295,  5.24335295,  5.04560255,\n",
       "        3.38732798,  2.0806368 ,  0.70006663,  4.50763181, -0.87055345,\n",
       "        7.9679862 ,  2.2783872 ,  6.08595238,  3.80863649,  4.7130827 ,\n",
       "        9.54630677,  5.04560255,  6.08595238,  4.72594438,  4.50763181,\n",
       "        8.28764437,  0.50231623,  4.64599038,  6.08595238,  3.38732798,\n",
       "        6.08595238,  6.30426495,  2.0144583 ,  2.0144583 ,  5.24335295,\n",
       "        5.04560255,  6.08595238,  3.46728198,  5.76629421,  8.28764437,\n",
       "        5.04560255,  5.04560255,  3.46728198,  0.70006663,  6.30426495,\n",
       "        3.46728198,  8.28764437,  2.76828665,  5.68634022,  5.04560255,\n",
       "        3.46728198,  5.68634022,  6.30426495,  3.46728198,  2.0944123 ,\n",
       "        4.50763181,  5.04560255,  2.0144583 ,  6.30426495,  4.7130827 ,\n",
       "        8.28764437,  6.08595238,  3.46728198,  1.50962425,  4.50763181,\n",
       "        5.04560255,  6.08595238,  2.76828665,  4.50763181,  6.08595238,\n",
       "        5.24335295,  5.04560255,  7.34461479,  5.04560255,  4.50763181,\n",
       "        0.50231623,  8.28764437,  4.50763181,  4.50763181,  4.7130827 ,\n",
       "        2.0144583 ,  3.67273287,  2.76828665,  4.50763181,  6.08595238,\n",
       "        2.0806368 ,  4.42767782,  6.08595238,  3.66503238,  1.1771044 ,\n",
       "        8.28764437,  2.0144583 ,  4.64599038,  6.08595238,  3.13476213,\n",
       "        6.30426495,  5.68634022,  0.50231623,  4.50763181,  5.04560255,\n",
       "       -0.1966791 ,  4.50763181,  6.30426495,  3.67273287,  5.24335295,\n",
       "        4.70538221,  6.08595238,  4.50763181,  4.50763181,  6.08595238,\n",
       "        1.50962425,  9.54630677,  4.72594438,  6.08595238,  5.68634022,\n",
       "        9.54630677,  3.38732798,  5.76629421,  3.67273287,  4.50763181,\n",
       "        5.04560255,  8.28764437,  0.50231623,  4.64599038,  4.42767782,\n",
       "        0.50231623,  8.28764437,  6.30426495,  5.04560255,  8.28764437,\n",
       "        6.30426495,  3.3392992 ,  5.04560255,  6.30426495,  6.08595238,\n",
       "        6.30426495,  5.76629421,  6.30426495,  1.50962425,  6.08595238,\n",
       "        5.24335295,  3.66503238,  5.68634022,  3.05480814,  0.50231623,\n",
       "        4.70538221,  8.28764437,  5.68634022,  7.9679862 ,  5.24335295,\n",
       "        5.04560255,  5.24335295,  6.30426495,  4.72594438,  5.04560255,\n",
       "        5.04560255,  3.46728198,  6.08595238,  3.38732798,  6.30426495,\n",
       "        5.24335295,  2.0944123 ,  2.76828665,  2.0144583 ,  6.30426495,\n",
       "        6.08595238,  6.7093238 ,  5.33645412,  6.08595238,  6.7093238 ,\n",
       "        4.70538221,  2.76828665,  5.24335295,  1.76097863,  5.24335295,\n",
       "        5.24335295,  4.50763181,  5.04560255,  5.68634022,  6.30426495,\n",
       "        3.67273287,  5.24335295,  3.66503238,  6.28370279,  0.70006663,\n",
       "        3.66503238,  4.64599038,  3.05480814,  4.64599038,  2.76828665,\n",
       "        3.3392992 ,  5.24335295,  2.0944123 ,  4.72594438,  6.9070742 ,\n",
       "        5.24335295,  0.50231623,  2.0944123 ,  6.08595238,  7.9679862 ,\n",
       "        0.70006663,  3.66503238,  6.30426495,  2.0944123 ,  3.67273287,\n",
       "        8.28764437,  4.50763181,  3.46728198,  4.50763181,  6.08595238,\n",
       "        3.38732798,  3.13476213,  8.28764437,  3.67273287,  2.0944123 ,\n",
       "        0.50231623,  4.50763181,  6.08595238,  7.34461479,  1.50962425,\n",
       "        5.04560255,  3.66503238,  0.13675457,  5.04560255,  4.50763181,\n",
       "        2.0944123 ,  6.08595238,  1.50962425,  4.72594438,  5.04560255,\n",
       "        9.54630677,  3.66503238,  5.04560255,  5.04560255,  0.70006663,\n",
       "        5.04560255,  3.66503238,  6.30426495,  5.04560255,  9.54630677,\n",
       "        8.28764437,  5.04560255,  2.0944123 ,  5.76629421,  5.04560255,\n",
       "        6.08595238,  5.04560255,  4.72594438,  5.76629421,  5.04560255,\n",
       "        8.28764437,  6.30426495,  2.0144583 ,  0.50231623,  6.08595238,\n",
       "        4.72594438,  1.50962425,  2.0944123 ,  5.04560255,  3.38732798,\n",
       "        2.2783872 ,  5.04560255,  7.88803221,  3.66503238,  1.50962425,\n",
       "        6.30426495,  2.76828665,  5.24335295,  4.50763181,  5.24335295,\n",
       "        6.08595238,  2.76828665,  3.38732798,  3.38732798,  5.04560255,\n",
       "        4.70538221,  1.50962425,  3.38732798,  4.50763181,  5.04560255,\n",
       "        3.58507838,  3.66503238,  6.30426495,  6.30426495,  9.54630677,\n",
       "        5.24335295,  0.70006663,  4.7130827 ,  6.30426495,  3.67273287,\n",
       "        4.72594438,  2.0144583 ,  7.88803221,  7.9679862 ,  6.30426495,\n",
       "        7.9679862 ,  3.46728198,  4.72594438,  2.76828665,  1.50962425,\n",
       "        8.28764437,  6.30426495,  5.04560255,  5.04560255,  6.08595238,\n",
       "        4.72594438,  4.50763181,  6.30426495,  6.30426495,  8.28764437,\n",
       "        5.24335295,  3.67273287,  0.70006663,  2.0144583 ,  6.30426495])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "y_pred = gbrt.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 7.35895865,  4.51829824,  5.75315756,  4.80321222,  5.55639636,\n",
       "        2.69048886,  5.88604093,  2.33405955,  6.42725096,  5.05489882,\n",
       "       -0.28768336,  5.39020486,  8.28733763,  5.56905071,  5.50418408,\n",
       "        5.0676613 ,  2.39335412,  4.4962902 ,  9.39923963,  5.42334091,\n",
       "        5.89258629,  3.62117368,  7.51386256,  0.69505292,  3.48098645,\n",
       "        6.00593153,  4.12698593,  2.31115204,  5.61310284,  3.7222933 ,\n",
       "        6.78417047,  6.98097237,  4.79283027,  5.98337452,  5.46264371,\n",
       "        5.63953086,  4.46557414,  6.05260543, -0.33318674,  9.18171176,\n",
       "        3.00238531,  4.6902385 ,  6.23504058,  5.29277334,  5.67022626,\n",
       "        5.55261842,  5.1723993 ,  2.29734098,  8.36884886,  4.80073954,\n",
       "        1.76468473,  1.52559302,  5.05463184,  4.85940594,  4.85846744,\n",
       "        1.7412035 ,  2.81092055,  0.36327708,  0.98146289,  0.59283957,\n",
       "        6.90013143,  5.0152344 ,  6.49715797,  2.62763294,  4.06137369,\n",
       "        7.69931304,  5.21195631,  6.10536628,  7.07609797,  1.47051235,\n",
       "        9.30027744,  1.33494354,  1.75319075,  7.0505187 ,  2.08254552,\n",
       "        5.79202758,  6.60125384,  0.60370327,  4.2917403 ,  5.01279148,\n",
       "        5.61593549,  5.87298572,  2.84293178,  6.20017435,  7.52541193,\n",
       "        5.72635619,  6.8451318 ,  2.57862796,  0.39799249,  3.78991518,\n",
       "        3.88719182,  8.61676228,  2.84000635,  5.79492034,  2.03520402,\n",
       "        2.44393269,  5.0972549 ,  7.53172593,  3.39664182,  1.20000113,\n",
       "        6.47630579,  7.20223437,  3.51276773,  7.00414535,  3.44555316,\n",
       "        8.67151376,  5.6786728 ,  4.47287677,  2.26756308,  0.86786346,\n",
       "        7.38932955,  3.86675743,  2.33740512,  5.66386582,  5.93637676,\n",
       "        5.63763059,  6.32172739,  5.37157094,  2.47130495,  3.46130269,\n",
       "       -0.44904936,  3.96269677,  6.18718412,  4.82420423,  4.37616246,\n",
       "        0.14643026,  5.28131986,  3.43207589,  3.1832051 ,  1.24502112,\n",
       "        1.90129278,  5.65127794,  5.39775411,  5.09709945,  3.39652017,\n",
       "        9.38562356,  2.1889959 ,  3.7689892 ,  4.97904422,  3.93097033,\n",
       "        6.02185072,  5.53833331,  1.21832015,  0.90583002,  3.44484525,\n",
       "        6.0042971 ,  5.25140857,  7.13249721,  1.86989188,  4.98748117,\n",
       "        0.50629598,  5.49862321,  4.47937839,  5.37137757,  6.39740962,\n",
       "        1.67214696,  7.12814653,  6.69523092,  5.6449585 ,  7.08416416,\n",
       "        7.51936763,  5.01846637,  6.49238062,  4.63937858,  4.51230749,\n",
       "        4.96510593,  7.90996675, -0.20562154,  3.07255309,  6.04989577,\n",
       "       -1.17548011,  8.31827738,  5.34492273,  1.74668015,  9.15014334,\n",
       "        5.43903618,  7.46769911,  5.48436264,  6.89496823,  6.92632799,\n",
       "        7.48005027,  5.70831798,  6.20094876,  2.31442437,  7.47843035,\n",
       "        4.8478958 ,  4.07737977,  4.88756327,  2.36746943,  0.91148504,\n",
       "        6.25720416,  8.95836703,  5.46359059,  7.17321747,  5.2670419 ,\n",
       "        4.56011914,  6.36003643,  6.75244398,  2.97609887,  2.92325282,\n",
       "        4.47702778,  3.79475983,  6.48470876,  1.97375607,  6.03335027,\n",
       "        4.89341854,  0.8115953 ,  2.32875865,  2.8800147 ,  7.96775064,\n",
       "        8.2501586 ,  7.88137188,  4.04994415,  5.25943151,  4.05941416,\n",
       "       -0.06601914,  1.38443333,  4.52860506,  0.62077563,  4.67452267,\n",
       "        4.78131126,  7.10737029,  5.93407438,  6.85829354,  4.96976974,\n",
       "        4.0486145 ,  7.69618427,  4.35810257,  5.58149935,  0.14222455,\n",
       "        5.95350169,  6.89273284,  2.88051805,  2.44583153,  3.56541191,\n",
       "        5.01732618,  4.22144263,  0.54099755,  3.51890521,  1.40330855,\n",
       "        2.87191182,  1.32837185,  1.55172831,  6.79995599,  7.42343233,\n",
       "        0.23847304,  3.74978296,  4.81919719,  1.42977319,  2.41047494,\n",
       "        8.65868116,  1.87246116,  2.97141522,  5.73206458,  6.05571361,\n",
       "        2.53650794,  2.91950834,  9.45762688,  1.91076161,  3.31999091,\n",
       "        2.18092503,  4.4612457 ,  7.15883835,  6.51737556,  2.62219799,\n",
       "        3.10714235,  4.67411419,  0.79349205,  2.46506368,  6.56491214,\n",
       "        3.62096207,  6.70834881,  1.51022218,  4.31500308,  5.35158371,\n",
       "        8.24987481,  4.73780021,  6.33142057,  2.53586417,  0.09891303,\n",
       "        4.08313001,  4.91706598,  3.44311201,  7.50888897,  9.1441185 ,\n",
       "        8.73345926,  5.15839807,  2.34626758,  6.29598626,  1.89160157,\n",
       "        6.37434181,  7.32758431,  5.29754988,  7.10793999,  1.32079241,\n",
       "        9.01314071,  3.74393508,  1.69535095, -0.07894094,  5.29886989,\n",
       "        5.14615031,  1.9987863 ,  3.7007501 , 10.11250474,  4.28857072,\n",
       "        5.76054477,  2.30245046,  7.27348455,  4.12457326,  0.53610949,\n",
       "        5.81394084,  1.13628321,  5.10768018,  1.21705416,  6.36348857,\n",
       "        7.00992449,  4.65422237,  1.98456121,  3.53118801,  6.99204038,\n",
       "        5.43441353,  0.08686823,  1.04544493,  0.25007181,  1.77992226,\n",
       "        3.9533375 ,  4.7441981 ,  4.37699257,  5.29004555,  6.70070888,\n",
       "        4.77441312,  0.41352199,  4.31019451,  3.9945362 ,  3.42425434,\n",
       "        5.52227573,  1.15139149,  6.46094464,  7.5443125 ,  6.66511794,\n",
       "        8.29950073,  4.55368355,  2.9273129 ,  1.18876073,  1.18578085,\n",
       "        6.58686428,  6.27869211,  6.51396399,  8.02154566,  5.94372487,\n",
       "        4.75869731,  2.43994818,  7.00847208,  6.90201492,  8.62989861,\n",
       "        4.83298071,  1.50069763,  0.81049795,  2.79618961,  4.65916439])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_slow.fit(X_train, y_train)\n",
    "y_pred = gbrt_slow.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "### Gradient Boosting with Early Stopping\n",
    "\n",
    "The examples above demonstrate ensembles with a different number of estimators. Which one is better? How can you quantitatively estimate the optimal number of predictors?\n",
    "\n",
    "One technique that you can use is called *early stopping* – it allows your algorithm to stop as soon as the validation error reaches a minimum. An easy way to implement this with `sklearn` is to use `staged_predict()` method as the code below demonstrates:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.2,\n            train_size=None)\nTRAIN: 1437 TEST: 360\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=117, random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=100)\n",
    "X, y = digits[\"data\"], digits[\"target\"]\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "split.get_n_splits(X, y)\n",
    "print(split)       \n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "bst_n_estimators # optimal number of trees"
   ]
  },
  {
   "source": [
    "Finally, an alternative to training a large number of estimators and then looking back in order to find the optimal number of those is to allow the algorithm to stop training when the validation error does not improve over a number of consecutive iterations (e.g., $5$ in the code below). For that to work, set the `warm_start` to `True`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping\n",
    "\n",
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}